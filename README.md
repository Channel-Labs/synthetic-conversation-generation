<!-- PROJECT LOGO -->
<br />
<div align="center">

<h3 align="center">Synthetic Conversation Generation </h3>

  <p align="center">
Generate conversations between your AI and synthetic users to test, refine, and improve your models.    
    <br />
  </p>
</div>

<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#about-the-project">About The Project</a>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
      <ul>
        <li><a href="#prerequisites">Prerequisites</a></li>
        <li><a href="#installation">Installation</a></li>
      </ul>
    </li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#examples">Examples</a></li>
    <li><a href="#roadmap">Roadmap</a></li>
    <li><a href="#contributing">Contributing</a></li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
  </ol>
</details>


<!-- ABOUT THE PROJECT -->
## About The Project

### What is Synthetic Conversation Generation?
When you build a multi-turn conversational AI, testing and evaluation across a wide range of user interactions is critical. However, most products don't have enough real users to generate the volume and diversity of conversations needed for effective testing.

Synthetic conversation generation allows you to simulate realistic dialogues between your AI and a diverse set of synthetic users. By generating these conversations at scale, you can better understand user experiences, identify gaps in your AI's behavior, and systematically improve performance without requiring access to large volumes of real user data.

### What Makes This Repo Unique?
Many libraries for generating synthetic data for LLMs exist, as outlined in Awesome-LLM-Synthetic-Data. However, existing tools often fall short in producing realistic, high-fidelity multi-turn conversations. This library aims to bridge that gap through the following innovations:

Many libraries for generating synthetic data for LLMs exist, as outlined in [Awesome-LLM-Synthetic-Data](https://github.com/wasiahmad/Awesome-LLM-Synthetic-Data). However, existing tools often fall short in producing realistic multi-turn conversations. This library aims to bridge that gap through the following innovations:

- **Scalable and Diverse Conversations**: Synthetic datasets often fail to replicate the statistical distributions and contextual complexity of real user interactions, leading to models that overfit synthetic patterns and underperform on real users. Furthermore, synthetic data generated by LLMs can inadvertently perpetuate demographic and behavioral biases if diversity is not explicitly enforced. This library addresses these challenges by decoupling persona generation from conversation generation. Personas are created sequentially, with each new persona explicitly instructed to differ from previously generated ones, ensuring broad diversity before conversation generation begins.
- **Realistic Conversation Stopping Points**: Most synthetic dialogue systems either let conversations last too long or truncate them at an arbitrary turn limit, resulting in unnatural dialogues. This library models natural conversation termination by dynamically evaluating if the user's goal has been achieved or if the user has become frustrated, and ending the conversation accordingly.

### What does this repo do?

- **User Persona Generation**: Automatically creates a diverse set of realistic user personas based on the AI assistantâ€™s specifications, capturing a range of user intents, behaviors, and backgrounds.
- **Conversation Simulation**: Simulates multi-turn dialogues between the AI assistant and each user persona to support robust testing and refinement.

<!-- GETTING STARTED -->
## Getting Started

Here's how to set up the Synthetic Conversation Generation toolkit.

### Prerequisites

* Python 3.7+
* pip (Python package manager)
* API keys for your chosen LLM provider 
  - [OpenAI](https://platform.openai.com/docs/overview)
  - [Anthropic](https://www.anthropic.com/api)

### Installation

1. Clone the repo
   ```sh
   git clone https://github.com/channel-labs/synthetic-conversation-generation.git
   ```
2. Install Python dependencies
   ```sh
   pip install -r requirements.txt
   ```
3. Set up environment variables for your API keys
   ```sh
   # If using OpenAI
   export OPENAI_API_KEY='your_openai_api_key'

   # If using Anthropic
   export ANTHROPIC_API_KEY='your_anthropic_api_key'
   ```

<!-- USAGE EXAMPLES -->
## Usage

The toolkit provides two main functionalities:

### 1. Generate Event Schema

First, you can use this repo generate a schema that defines what events and properties to track in your conversations:

```sh
python src/generate_schema.py \
  --data-path examples/therapist/example_data.json \
  --data-schema-output-path therapist_schema.yml \
  --model-provider openai \
  --assistant-namer-model gpt-4.1 \
  --event-schema-model o3-mini
```

This will:
- Analyze your conversation data
- Generate a schema defining meaningful events and properties
- Save it to the specified output file

Feel free to modify the resultant schema as needed. We've found the best results occur when domain experts use their expertise to improve upon the LLM's suggested schema.

### 2. Upload Events

After generating a schema, you can process conversations and upload the tagged events to your analytics platform:

```sh
python src/upload_events.py \
  --data-path examples/therapist/example_data.json \
  --data-schema-path therapist_schema.yml \
  --destination posthog \
  --model-provider openai \
  --event-model gpt-4.1
```

This will:
- Process conversations according to your schema
- Use LLMs to identify and tag events
- Upload the events to your chosen analytics platform (Amplitude or PostHog)

### Examples

The tool supports conversation data in multiple formats. See the examples/ directory for examples

- **JSON format**: Structured conversation data where:
  - Each key is a conversation_id
  - Each value is an object with a `messages` array
  - Each message requires `role` and `content` fields
  - Optional message fields: `timestamp` and `message_id`

- **CSV format**: Tabular conversation data with the following columns:
  - Required: either `user_id` or `conversation_id` (at least one is needed)
  - Required: `role` and `content` columns
  - Optional: `message_id` and `timestamp` columns

Data can be loaded from:

- **Local files**: Direct path to your JSON or CSV files
- **Amazon S3**: Use s3:// URI format (e.g., `s3://your-bucket/path/to/conversations.json`)

The repository includes two example datasets to help you get started:

1. **Mental Health Companion** (`examples/therapist/`)
   - Example data: Conversations between users and a therapy assistant
   - Run schema generation:
     ```sh
     python src/generate_schema.py \
       --data-path examples/therapist/example_data.json \
       --data-schema-output-path therapist_schema.yml \
       --model-provider openai \
       --assistant-namer-model gpt-4.1
     ```
   - Run event tagging and upload:
     ```sh
     python src/upload_events.py \
       --data-path examples/therapist/example_data.json \
       --data-schema-path examples/therapist/schema.yml \
       --destination posthog

2. **Tax Advisor** (`examples/tax_advisor/`)
   - Example data: Conversations between users and a tax assistance bot
   - Run schema generation:
     ```sh
     python src/generate_schema.py \
       --data-path examples/tax_advisor/example_data.json \
       --data-schema-output-path tax_advisor_schema.yml
     ```
   - Run event tagging and upload:
     ```sh
     python src/upload_events.py \
       --data-path examples/tax_advisor/example_data.json \
       --data-schema-path examples/tax_advisor/schema.yml \
       --destination posthog
     ```

For S3 data sources, ensure you have AWS credentials configured and use the S3 URI format:

```sh
python src/generate_schema.py \
  --data-path s3://your-bucket/conversations/data.json \
  --data-schema-output-path my_schema.yml \
  --model-provider openai
```

<!-- ROADMAP -->
## Roadmap

- [x] Schema generation
- [x] Event tagging with LLMs
- [x] Analytics platform integration (Amplitude, PostHog)
- [ ] Built-in analytics dashboard
- [ ] Conversation improvement recommendations
- [ ] Support for more LLM providers and analytics platforms

<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

If you have a suggestion that would make this better, please fork the repo and create a pull request. Otherwise, feel free to start a discussion or open an issue here on GitHub, and we'll review shortly.

Don't forget to give the project a star! Thanks again!

<!-- LICENSE -->
## License

Distributed under the MIT License. See `LICENSE` for more information.

<!-- CONTACT -->
## Contact

Create by [Channel Labs](https://channellabs.ai/)

Interested in understand and improving your AI's behavior even further? Contact scott@channellabs.ai for any inquiries.
