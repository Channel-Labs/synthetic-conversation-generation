<!-- PROJECT LOGO -->
<div align="center">

<h1 align="center">Synthetic Conversation Generation </h1>

  <p align="center">
Generate conversations between your AI and synthetic users to test, refine, and improve your models.    
    <br />
  </p>
</div>

<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li>
      <a href="#about-the-project">About The Project</a>
    </li>
    <li>
      <a href="#getting-started">Getting Started</a>
      <ul>
        <li><a href="#prerequisites">Prerequisites</a></li>
        <li><a href="#installation">Installation</a></li>
      </ul>
    </li>
    <li><a href="#usage">Usage</a></li>
    <li><a href="#contributing">Contributing</a></li>
    <li><a href="#license">License</a></li>
    <li><a href="#contact">Contact</a></li>
  </ol>
</details>


<!-- ABOUT THE PROJECT -->
## About The Project

### What is Synthetic Conversation Generation?
When you build a multi-turn conversational AI, testing and evaluation across a wide range of user interactions is critical. However, most products don't have enough real users to generate the volume and diversity of conversations needed for effective testing.

Synthetic conversation generation allows you to simulate realistic dialogues between your AI and a diverse set of synthetic users. By generating these conversations at scale, you can better understand user experiences, identify gaps in your AI's behavior, and systematically improve performance without requiring access to large volumes of real user data.

### What Makes This Repo Unique?
Many libraries for generating synthetic data for LLMs exist, as outlined in [Awesome-LLM-Synthetic-Data](https://github.com/wasiahmad/Awesome-LLM-Synthetic-Data). However, existing tools often fall short in producing realistic multi-turn conversations. This library aims to bridge that gap through the following innovations:

- **Scalable and Diverse Conversations**: Synthetic datasets often fail to replicate the statistical distributions and contextual complexity of real user interactions, leading to models that overfit synthetic patterns and underperform on real users. Furthermore, synthetic data generated by LLMs can inadvertently perpetuate demographic and behavioral biases if diversity is not explicitly enforced. This library addresses these challenges by decoupling persona generation from conversation generation. Personas are created sequentially, with each new persona explicitly instructed to differ from previously generated ones, ensuring broad diversity before conversation generation begins.
- **Realistic Conversation Stopping Points**: Most synthetic dialogue systems either let conversations last too long or truncate them at an arbitrary turn limit, resulting in unnatural dialogues. This library models natural conversation termination by dynamically evaluating if the user's goal has been achieved or if the user has become frustrated, and ending the conversation accordingly.

### What does this repo do?

- **User Persona Generation**: Automatically creates a diverse set of realistic user personas based on the AI assistant's specifications, capturing a range of user intents, behaviors, and backgrounds.
- **Conversation Simulation**: Simulates multi-turn dialogues between the AI assistant and each user persona to support robust testing and refinement.

<!-- GETTING STARTED -->
## Getting Started

Here's how to set up the Synthetic Conversation Generation toolkit.

### Prerequisites

* Python 3.9+
* pip (Python package manager)
* API keys for your chosen LLM provider 
  - [OpenAI](https://platform.openai.com/docs/overview)
  - [Anthropic](https://www.anthropic.com/api)

### Installation

1. Clone the repo
   ```sh
   git clone https://github.com/channel-labs/synthetic-conversation-generation.git
   ```
2. Install Python dependencies
   ```sh
   pip install -r requirements.txt
   ```
3. Install the package in development mode
   ```sh
   pip install -e .
   ```
4. Set up environment variables for your API keys
   ```sh
   # If using OpenAI
   export OPENAI_API_KEY='your_openai_api_key'

   # If using Anthropic
   export ANTHROPIC_API_KEY='your_anthropic_api_key'
   ```

<!-- USAGE EXAMPLES -->
## Usage

### 0. Assistant Definition

Before generating personas or conversations, you need to create a YAML file that defines your AI assistant. This file should contain the assistant's name and description.

**Assistant YAML Format:**

```yaml
name: "Your Assistant Name"
description: "Brief description of your assistant's purpose and capabilities"
```

**Example (`data/assistants/fashion_advisor.yaml`):**

```yaml
name: "Fashionable Fran"
description: "Fashionable Fran is your personal stylist and recommends what to wear from your wardrobe."
```

### 1. User Persona Generation

Generate a diverse set of realistic user personas tailored to your AI assistant. These personas capture a range of backgrounds, goals, and interaction styles, enabling robust and representative conversation simulations.

**Usage:**

```sh
python src/synthetic_conversation_generation/persona_generator.py \
  --assistant-path <ASSISTANT_PATH> \
  --num-personas <NUM_PERSONAS> \
  --output-path <OUTPUT_PATH> \
  --model-provider <MODEL_PROVIDER> \
  --model-id <MODEL_ID> \
  --previous-personas-path <PREVIOUS_PERSONAS_PATH>
```

**Arguments:**

- `--assistant-path`: Path to YAML file containing your assistant definition (name and description).
- `--num-personas`: Number of user personas to generate (default: `5`).
- `--output-path`: Path to save the generated personas (YAML format).
- `--model-provider`: LLM provider to use for generating personas (`openai` or `anthropic`, default: `openai`).
- `--model-id`: Model ID for persona generation (default: `o3`).
- `--previous-personas-path`: Path to YAML file containing previous personas to avoid duplication (optional).

**Example:**

```sh
python src/synthetic_conversation_generation/persona_generator.py \
  --assistant-path data/assistants/fashion_advisor.yaml \
  --num-personas 3 \
  --output-path data/conversation_characters/fashion_advisor_personas.yaml
```

After generation, review and optionally edit the personas in the output YAML file to ensure they fit your use case.

### 2. Conversation Simulation

Generate realistic conversations between synthetic users and your AI assistant. The system intelligently creates user messages that match the personas from step 1, calls your AI endpoint for responses, and determines natural conversation endpoints. Unlike typical synthetic data generators, this system dynamically evaluates whether a conversation should continue or end naturally. After each turn, it determines if the user's needs have been met or if the conversation has reached a logical conclusion, creating more realistic dialogue patterns.

```sh
python src/synthetic_conversation_generation/conversation_generator.py \
  --assistant-path <ASSISTANT_PATH> \
  --conversation-characters-path <CONVERSATION_CHARACTERS_PATH> \
  --inference-endpoint-path <INFERENCE_ENDPOINT_PATH> \
  --output-path <OUTPUT_PATH> \
  --model-provider <MODEL_PROVIDER> \
  --model-id <MODEL_ID> \
  --conversation-completion-query-model-id <CONVERSATION_COMPLETION_MODEL_ID> \
  --max-conversation-turns <MAX_CONVERSATION_TURNS>
```

**Arguments:**

- `--assistant-path`: Path to YAML file containing your assistant definition (name and description).
- `--conversation-characters-path`: Path to the YAML file containing user personas (output from persona_generator).
- `--inference-endpoint-path`: Path to a YAML file specifying how to call your AI assistant via HTTP.
- `--output-path`: Path to save the generated conversations (JSONL format).
- `--model-provider`: LLM provider to use for generating user messages (`openai` or `anthropic`, default: `openai`).
- `--model-id`: Model ID for generating user messages (default: `gpt-4o`).
- `--conversation-completion-query-model-id`: Model ID for determining when conversations should end (default: `o3`).
- `--max-conversation-turns`: Maximum number of turns a conversation can have (default: `3`).

**Example:**

```sh
python src/synthetic_conversation_generation/conversation_generator.py \
  --assistant-path data/assistants/fashion_advisor.yaml \
  --conversation-characters-path data/conversation_characters/fashion_advisor_personas.yaml \
  --inference-endpoint-path data/endpoint/openai_chat_completion.yaml \
  --output-path data/conversations/fashion_advisor_conversations.jsonl
```

<!-- CONTRIBUTING -->
## Contributing

Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

If you have a suggestion that would make this better, please fork the repo and create a pull request. Otherwise, feel free to start a discussion or open an issue here on GitHub, and we'll review shortly.

Don't forget to give the project a star! Thanks again!

<!-- LICENSE -->
## License

Distributed under the MIT License. See `LICENSE` for more information.

<!-- CONTACT -->
## Contact

Created by [Channel Labs](https://channellabs.ai/)

Interested in understanding and improving your AI's behavior even further? Contact scott@channellabs.ai for any inquiries.
